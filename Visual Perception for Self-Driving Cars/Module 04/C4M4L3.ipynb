{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descendent \n",
    "\n",
    "- Gradiente descent is an optimization algorithm used in machine learning to minimize a loss function (or cost function) by iteratively updating model parameters (weights and biases). \n",
    "\n",
    "- The process involves calcilating the gradient (partial derivative) of the loss function with respect to the parameters to determine the direction of steepest ascent. The parameters are then udpdated in the opposite direction (Descent) by a small step, called the learning rate. \n",
    "\n",
    "- This continues until the loss function converged to a minimum, ideally leading to the optimal model. Variantes include batch gradient descent(using the whole dataset), Stochastic gradient descent, and mini-batch gradient descent. \n",
    "\n",
    "## Summary\n",
    "\n",
    "- You can optimize for neural network parameters using batch gradient descent\n",
    "- When the number of training examples is extremely large, you can minibatch (stochastic) gradient descent.\n",
    "- State-of-the-art optimization algorithms built on top of SGD are implemented in most neural network libraries. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
