{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148e7128-0be5-4ca7-a1c5-63747c2f95ce",
   "metadata": {},
   "source": [
    "# 3D Geometry and Reference Frames\n",
    "\n",
    "## Sensors:\n",
    "\n",
    "- IMU: Inertial Measurement Unit\n",
    "- GNSS: Global Navigation Satellite Systems\n",
    "\n",
    "## Reference Frames | ECIF: Earth Centered Inertial Equatorial\n",
    "\n",
    "- ECIF coordinate frame is fixed.\n",
    "- Earth rotates about the z-axis.\n",
    "\n",
    "## Reference Frames | ECEF: Eath Centered Earth Fixed\n",
    "\n",
    "- ECEF coordinate frame rotates with the earth\n",
    "\n",
    "## Reference Frames | Navigation: NED\n",
    "\n",
    "- x: True north\n",
    "- y: True East\n",
    "- z: Down (Along with Gravity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0baf31-a555-45df-82e5-66b35384e959",
   "metadata": {},
   "source": [
    "# The inertial Measurements Unit(IMU)\n",
    "\n",
    "- The inertial measurement unit, or IMU measures the movement of a body in inertial space.\n",
    "- The IMU is typically composed of\n",
    "1. Gyroscopes: Measure angular rotation rates about three separate axes\n",
    "2. Accelerometters: measure accelerations along three orthogonal axes\n",
    "\n",
    "## Summary\n",
    "\n",
    "- A 6 DOF IMU is composed of three gyroscopes and three aceelerometers, mounted orthogonally.\n",
    "- A strapdown gyroscope measures a rotational rate in the sensor frame.\n",
    "- A strapdown accelerometer measures a specific force (or accelertion relative to free-fall) in the sensor frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2ab81-7176-43c4-9b9b-2aca7d0615e0",
   "metadata": {},
   "source": [
    "# The Global Navigation Satellite Systems (GNSS)\n",
    "\n",
    "- It's able to provide a position fix anywhere in the world with bounded error which is key.\n",
    "- Global Navigation Satellite System (GNSS): Is a catch-all term for a satellite system(s) that can be used to pinpoint a receiver's position anywhere in the world.\n",
    "\n",
    "## GPS: Global Positioning Systems\n",
    "\n",
    "- Composed of 24 to 32 sastellites in 6 orbital planes.\n",
    "- Each satellite broadcasts on two frequencies.\n",
    "\n",
    "## Computing Position\n",
    "\n",
    "- Each GPS satellite transmits a signal that encodes\n",
    "1. Its position: Via accurate ephemeris information.\n",
    "2. Time of signal transmission\n",
    "\n",
    "- To compute a GPS position fix in the Earth-centred frame, the receiver uses the speed of light to compute distances to each satellite based on time of signal arrival.\n",
    "- At least four satellites are required to solve for 3D position, three is only 2D is required.\n",
    "\n",
    "## GPS | Error Sources\n",
    "\n",
    "- Lonospheric delay: Charged ions in the atmosphere affect signal propagation.\n",
    "- Multipath effects: Surrounding terrain, buildings can cause unwanted reflection.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- A GNSS works through trilateration via pseudoranging from at least 4 satellites for a 3D position fix.\n",
    "- GNSS error can be caused by ionospheric delays, multipath effects, and precision is also affected by GDOP.\n",
    "- For GPS, differential GPS and RTK GPS are potential methods to substantially improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633cf23c-a1d9-48cd-b3c7-73d2efdbf08f",
   "metadata": {},
   "source": [
    "# Sensor Fusion\n",
    "\n",
    "# Sensor Fusion in Autonomous Driving\n",
    "\n",
    "**Sensor fusion** is the process of integrating data from multiple sensors to achieve a more accurate and reliable understanding of the surrounding environment. In the context of **autonomous driving**, this process plays a critical role in enabling self-driving cars to navigate safely and efficiently by creating a comprehensive understanding of the vehicle's surroundings.\n",
    "\n",
    "## Key Sensors in Autonomous Driving\n",
    "\n",
    "1. **Cameras**:\n",
    "   - **Function**: Capture detailed visual information (like road signs, lane markings, traffic lights, and obstacles).\n",
    "   - **Strengths**: High-resolution images and color information are great for recognizing objects and interpreting road scenes.\n",
    "   - **Limitations**: Limited by lighting conditions (e.g., night, fog, glare) and may struggle with depth perception compared to other sensors.\n",
    "\n",
    "2. **Lidar (Light Detection and Ranging)**:\n",
    "   - **Function**: Uses laser beams to measure distance to objects by calculating the time it takes for light to bounce back.\n",
    "   - **Strengths**: Provides accurate 3D maps of the surroundings, excellent for depth perception, and works well in various lighting conditions.\n",
    "   - **Limitations**: Expensive, can have limited range in bad weather (rain, snow), and struggles to detect some materials (e.g., glass).\n",
    "\n",
    "3. **Radar (Radio Detection and Ranging)**:\n",
    "   - **Function**: Uses radio waves to detect objects and measure their speed and distance.\n",
    "   - **Strengths**: Robust in poor weather (rain, fog, snow) and can detect objects at long distances. Excellent for detecting fast-moving objects.\n",
    "   - **Limitations**: Lower resolution than Lidar and cameras, making it less effective at identifying small or stationary objects.\n",
    "\n",
    "4. **Ultrasonic Sensors**:\n",
    "   - **Function**: Use sound waves to detect nearby objects, typically at low speeds for close-range detection.\n",
    "   - **Strengths**: Effective for parking and close-range obstacle detection (e.g., detecting curbs).\n",
    "   - **Limitations**: Short range, low resolution, and mainly suitable for low-speed maneuvers.\n",
    "\n",
    "5. **GPS (Global Positioning System)**:\n",
    "   - **Function**: Provides precise location data by receiving satellite signals.\n",
    "   - **Strengths**: Offers accurate positioning on a large scale and is vital for route planning.\n",
    "   - **Limitations**: Can be inaccurate in urban canyons or tunnels where satellite signals are blocked or reflected.\n",
    "\n",
    "6. **Inertial Measurement Unit (IMU)**:\n",
    "   - **Function**: Measures the vehicle's acceleration and rotational rates.\n",
    "   - **Strengths**: Provides information on the car’s movement and orientation, helping to keep track of position when GPS signals are weak.\n",
    "   - **Limitations**: Can drift over time and needs regular calibration.\n",
    "\n",
    "## Why Sensor Fusion?\n",
    "\n",
    "Each sensor type has its own **strengths and weaknesses**. For example, cameras can identify objects and read signs but struggle in poor weather, while radar works well in bad weather but can't provide fine-grained visual details. Sensor fusion combines the strengths of these different sensors to overcome their individual limitations, allowing the vehicle to:\n",
    "\n",
    "- **Improve accuracy**: Fusing Lidar's depth data with camera images gives a more detailed and accurate understanding of object shapes, sizes, and distances.\n",
    "- **Increase reliability**: Redundancy from multiple sensors ensures the system remains functional even if one sensor fails (e.g., when vision is impaired by fog, radar can still detect obstacles).\n",
    "- **Enhance perception**: Fusing radar and camera data can allow the system to recognize objects (such as pedestrians or other vehicles) and assess their speed and trajectory for collision avoidance.\n",
    "\n",
    "## Sensor Fusion Techniques\n",
    "\n",
    "1. **Low-Level (Data-Level) Fusion**:\n",
    "   - In this approach, raw data from different sensors (e.g., raw images, raw radar signals) are fused before any interpretation. The system processes the combined data to generate a richer dataset that is more informative than individual sensors alone.\n",
    "   - **Example**: Combining depth maps from Lidar with images from cameras to improve obstacle detection.\n",
    "\n",
    "2. **Mid-Level (Feature-Level) Fusion**:\n",
    "   - In mid-level fusion, individual sensors extract features (e.g., edges, objects, speed) before combining them into a single representation. This technique reduces the volume of data while still preserving important information.\n",
    "   - **Example**: Combining object recognition features from cameras with velocity data from radar to better track moving objects.\n",
    "\n",
    "3. **High-Level (Decision-Level) Fusion**:\n",
    "   - At this level, each sensor operates independently and makes decisions (e.g., detecting a pedestrian), and then the system merges the results to make the final decision.\n",
    "   - **Example**: Both radar and Lidar detect an object, and the final decision is based on the confidence scores from each sensor's output.\n",
    "\n",
    "## Applications of Sensor Fusion in Autonomous Driving\n",
    "\n",
    "1. **Object Detection and Classification**:\n",
    "   - Cameras and Lidar work together to detect and classify objects like pedestrians, vehicles, and road signs. Lidar provides depth information, while cameras provide visual details to classify what the object is.\n",
    "\n",
    "2. **Lane Detection and Road Understanding**:\n",
    "   - Cameras help detect lane markings, while radar and Lidar provide spatial information about the road and surroundings, ensuring the vehicle stays in the lane.\n",
    "\n",
    "3. **Obstacle Avoidance and Collision Prevention**:\n",
    "   - By combining radar’s long-range capabilities with the detailed object detection of cameras and Lidar, sensor fusion helps the vehicle detect potential hazards and avoid collisions.\n",
    "\n",
    "4. **Localization and Mapping**:\n",
    "   - GPS provides global positioning, while Lidar and cameras help create real-time maps of the local environment. IMU data can fill gaps when GPS signals are weak (e.g., in tunnels).\n",
    "\n",
    "## Challenges in Sensor Fusion\n",
    "\n",
    "1. **Data Alignment and Synchronization**:\n",
    "   - Each sensor has different data rates and resolutions. Aligning these data streams (e.g., matching timestamps) is a technical challenge in real-time systems.\n",
    "\n",
    "2. **Computational Complexity**:\n",
    "   - Processing data from multiple sensors in real time requires significant computational power, and ensuring low-latency decisions is critical for safety.\n",
    "\n",
    "3. **Data Uncertainty and Noise**:\n",
    "   - Sensors may provide noisy or incomplete data, which can lead to uncertainty in decisions. Handling this uncertainty through probabilistic models (like Kalman filters or Bayesian networks) is necessary for accurate fusion.\n",
    "\n",
    "4. **Cost and Integration**:\n",
    "   - Integrating a diverse array of sensors (e.g., Lidar, radar, cameras) can be expensive and technically complex. Striking a balance between cost and performance is a key issue in making autonomous vehicles commercially viable.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Sensor fusion is essential for enabling robust and reliable perception in autonomous vehicles. By combining the data from cameras, Lidar, radar, and other sensors, self-driving cars can achieve a more complete and accurate understanding of their environment, improving safety, navigation, and decision-making. While there are challenges in implementation, advances in machine learning and real-time processing are making sensor fusion increasingly effective for autonomous driving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7e9e9-2b42-40af-9466-28933aafce15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
